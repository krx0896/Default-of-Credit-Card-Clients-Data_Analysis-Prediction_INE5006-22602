# Project 
### 카드 채무 불이행 여부 예측 : 고객 카드 채무 데이터를 기반으로 채무 불이행 여부를 분류예측하기


# Intro 
### 🗓️ Date 
Project term : 2022.05.02 ~ 2022.06.12 </br>
Presentation Date : 2022.06.13 </br>
### :man: Professor 
  한양대학교 ERICA, 산업경영공학과 김병훈 교수님 
### 👥 Team member 
  * 산업경영공학과 김윤성
  * 산업경영공학과 김동익
  * 산업경영공학과 김규현
  * 산업경영공학과 정효림
  * 산업경영공학과 백재민
  * 산업경영공학과 정영훈
  
# Data Set 
### ✅ Source 
https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset <br/>
csv file : UCI_Credit_Card.csv : ( train + test )


### ✅ Characteristics of the dataset 
  * 타입 : Multivariate
  * 피쳐 : 25개 (Target var : ‘Yn’ , predictors : 24개)
  * 타겟 변수의 클래스 불균형 : Default한 사람의 비율 -> 78:22
  * 상관관계가 높은 변수들 존재

# Contents  

1.   서론: 문제 정의
1) 연구 배경
- 카드대란
2) 연구 목적
- 분석 목적
2.   데이터 사전 분석 결과
A.   분석 내용 소개
1) 변수 정보
- 범주형 변수와 수치형 변수 나눠서 설명(범주형: ID, SEX, EDUCATION, PAY0~6, DEFAULT & 수치형: LIMIT_BAL, AGE, BILL_AMT1~6, PAY_AMT1~6)
- 결측치, 이상치 파악(결측치는 없는것으로 확인, 모든 변수 BOXPLOT으로 이상치 확인 했더니 많이 보임 -> 전처리 필요)
2) 데이터 분포 확인
- 채무불이행 값 원그래프로 확인(클래스의 불균형으로 오버샘플링 해야겠다고 말하기, 오버샘플링으로 선택한 이유도 서술)
- 학력 변수는 고등 교육을 받을 수록 채무 불이행 가능성 낮아지는게 보이고 결혼 여부에 채무불이행이 미치는 영향이 미세한게 보이기 때문에 결혼 여부가 학력보다 덜 영향을 미치는 변수라는 것을 알 수 있다 -> 각각의 변수들에대한 변수 중요도를 파악할 필요가 있다고 언급
- 위의 이유 때문에 FEATURE IMPORTANCE를 확인한 결과 최근 3개월 내 변수가 종속 변수에 주는 영향이 3~6개월 변수들 보다 크게 나옴
- LIMIT_BAL 분포를 확인했더니 한도 범위(분포 그래프 가로축 0~600000)가 너무 넓어서 스케일링(표준화)을 해야겠다고 판단, 표준화를 하면 데이터가 어떤식으로 되고 왜 모델이 좋아지는지 충분한 이유 언급 
- 데이터가 상관관계가 있다면 다중공선성이 존재하게되어 상관관계분석을 진행함(다중공선성은 변수간 강한 상관관계가 존재하면 발생하고 분산이 높아 과적합을 유발하기때문에 상관관계 분석 진행함 언급)
B.   결과 요약
- 데이터분포를 확인한 결과 클래스의 뷸균형, 너무 넓은 범위의 데이터를 가진 변수, 변수들의 다른 중요도 가중치, 높은 상관관계를 가지는 변수들 발견
- 데이터 전처리때 오버샘플링, 스케일링, 중요도가 높은 변수들만 남긴 대조군 데이터셋 생성, 상관관계가 높은 변수들을 편차 평균으로 묶은 파생변수 만들기를 반영하기로 함 
- 이렇게 모델 예측력을 높여주는데 기여하는 요인들을 위 4가지로 설정한 후 설정한 4가지 요인이 실제로 영향이 있는지 알아보기위해 실험계획법의 분산분석을 통해 알아보기로함
3.   데이터 전처리
- 상관관계가 높은 변수(PAY_AMT, BILL_AMT)들을 말하고 이 변수들을 묶은 새로운 파생변수(1~6까지의 변수들을 편차 평균함, 파생변수를 편차 평균으로 만든 이유 언급) 추가한 대조군 1 데이터셋 만들었다 언급
- 최근 3개월 내 변수가 종속 변수에 주는 영향이 3~6개월 변수들 보다 크게 나와서 3~6개월 변수들을 뺀 대조군2 데이터셋 만들고 대조군2 데이터에서 편차 평균 파생변수를 만든 대조군3 데이터를 만듬
- 이렇게 생성된 오리지널 대조군1, 대조군2, 대조군3 데이터 셋들을 스케일링 한것 안한것으로 나누어 8개의 데이터 셋을 만듬
- 이렇게 만들어진 8개의 데이터셋을 스케일링 한것 안한것으로 나누어 16개의 데이터 셋을 만듬
- 데이터를 TRAIN, TEST SET 8대2 비율로 나눔
4.   모델 제안 (Framework)
1)   제안된 방법 소개
- 나이브 베이즈 모델 소개(나이브 베이즈 모델에 대한 설명)
- 로지스틱 회귀 모형 소개(로지스틱에 대한 설명)
- 랜덤 포레스트 모형 소개(랜덤 포레스트 모델 설명)
- XGB 모형 소개(XGB 모델 설명)
2)   제안한 모델이 앞서 분석한 내용과 어떤 연관이 있는지?
- 나이브 베이즈 모델 - 상관관계가 높은 변수들이 실제로 모델에 얼마나 영향을 끼칠지 알아보기 위해 독립여부에 성능이 좌우되는 나이브 베이즈 분류기 모델을 선택함
- 로지스틱 회귀 모델 - @우리 데이터와 뭐가 어울리며 왜 썻는지
- 랜덤 포레스트 모델 - 채무 불이행에 어떤 변수들이 영향을 많이 미치는지 순위를 매기기 위해 랜덤 포레스트 모델을 선택함
- XGB 모델 - 좀 더 생각해보기
5.   실험결과
1)   실험 과정 소개
- F1 스코어 사용 이유(먼저 모델 성능 평가를 위해 채무불이행자로 예측한 사람중 실제로 채무불 이행자에 대해 알 수 있는 정밀도와 실제 채무 불이행자 중 예측한 사람이 진짜 채무 불이행자일 확률인 재현율을 조화평균으로 측정해주어 종합적 성능평가가 가능한 F1 스코어를 1순위로 비교하였고 실제로 채무 이행자한테 불이행했다고 판단하는 경우가 회사에 타격이 더 크다고 판단하여 재현율을 2순위 지표로 잡고 각각 다른 총 8가지 데이터들을 분석하였습니다)
- 16개의 데이터 셋을 4개의 모델로 다 COMFUSION MATRIX를 써서 64개를 만들어 분석함
- 이때 제일 잘나온 모델을 채택하여 실험계획법을 통해 4개의 요인(스케일링, 오버샘플링, 파생변수 생성, 변수 삭제)들이 진짜 예측력을 높이는데 도움을 주었는가를 분석

2)   실험 결과 요약 및 원인 분석 
- 모델별 원본, 대조1, 대조2, 대조3 데이터를 파악후(피피티 오버 샘플링별 그래프) F1 스코어에 영향 주는 것 파악한 후 뭐가 왜 영향을 주는지 이유 분석(EX. 로지스틱에서는 오리지널 데이터와 대조군끼리의 차이는 별로 없었지만 오버 샘플링을 할때와 하지 않을때 점수 차이가 많이 나는 것을 확인했음, @ 다른 모델도 다 분석)
- 각 모델별 제일 잘나온 F1스코어의 COMFUSION MATRIX 4개 보여주고 왜 모델별로 제일 잘나온 데이터의 조건이 그때인지 이유 서술
- 모델 퍼포먼스 꺾은선 그래프로 비교(16개 데이터 셋 중에서 오버샘플링하고 스케일링 한 원본 데이터로 비교)
- 제일 F1 스코어가 잘나온 랜덤 포레스트 모델을 채택하여 4개의 요인(스케일링, 오버샘플링, 파생변수 생성, 변수 삭제)으로 분산분석 진행함
- 분산 분석 결과 스케일링과 오버샘플링이 가장 유의미하게 나옴(피피티 그림들 다 첨부)
6.   결론
1)   결과 요약
2)   프로젝트를 통해 느낀 점 간단히 요약
7.   참고문헌

# Result
### ✅ Source Code 
Colab -> 
데이터 EDA: https://colab.research.google.com/drive/1clQ4RM26VdxPXZf3_y-JffM2a7eObl0N?usp=sharing <br>
모델 로지스틱 회귀, 나이브 베이즈: https://colab.research.google.com/drive/1KQyxloWMmdR2RPW_xaON3C4C8Py19rAh?usp=sharing <br>
모델 랜덤포레스트, XGBOOST: https://colab.research.google.com/drive/1g4dHuVKoBtCY78IJcvtX26jXpW2PCnw3?usp=sharing

### ✅ Best Model & Score
RandomForest with Over Sampling & SMOTE </br>
  * recall : 0.84
  * F1socre : 0.5264
  * AUCscore : 0.7027
